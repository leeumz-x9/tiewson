// FaceAnalyzerPopup.js (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ã‡πâ‡∏≥)
import React, { useEffect, useRef, useState } from 'react';
import * as faceapi from 'face-api.js';
import { Camera, X } from 'lucide-react';
import { trackUserSession } from './analyticsService';

const FaceAnalyzerPopup = ({ onClose, onAnalysisComplete, language }) => {
  const videoRef = useRef(null);
  const canvasRef = useRef(null);
  const [isLoading, setIsLoading] = useState(true);
  const [modelsLoaded, setModelsLoaded] = useState(false);
  const [analyzing, setAnalyzing] = useState(true);
  const analysisTimeoutRef = useRef(null);
  const hasTrackedRef = useRef(false); // ‚≠ê ‡πÄ‡∏û‡∏¥‡πà‡∏° flag ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ã‡πâ‡∏≥

  useEffect(() => {
    const loadModels = async () => {
      try {
        const MODEL_URL = 'https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model';
        
        await Promise.all([
          faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
          faceapi.nets.ageGenderNet.loadFromUri(MODEL_URL),
          faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL)
        ]);
        
        setModelsLoaded(true);
        console.log('‚úÖ Face-API models loaded');
      } catch (err) {
        console.error('Error loading models:', err);
      }
    };

    loadModels();
  }, []);

  useEffect(() => {
    if (!modelsLoaded) return;

    const startVideo = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          video: { width: 1280, height: 720, facingMode: 'user' }
        });

        if (videoRef.current) {
          videoRef.current.srcObject = stream;
          videoRef.current.onloadedmetadata = () => {
            setIsLoading(false);
          };
        }
      } catch (err) {
        console.error('Error accessing webcam:', err);
        setIsLoading(false);
      }
    };

    startVideo();

    return () => {
      if (videoRef.current?.srcObject) {
        videoRef.current.srcObject.getTracks().forEach(track => track.stop());
      }
    };
  }, [modelsLoaded]);

  useEffect(() => {
    if (!modelsLoaded || isLoading) return;

    const detectFaces = async () => {
      // ‚≠ê ‡∏ñ‡πâ‡∏≤‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏µ‡∏Å
      if (hasTrackedRef.current) return;

      if (videoRef.current && videoRef.current.readyState === 4) {
        const video = videoRef.current;
        const canvas = canvasRef.current;

        if (canvas) {
          const displaySize = { width: video.videoWidth, height: video.videoHeight };
          faceapi.matchDimensions(canvas, displaySize);
        }

        try {
          const detections = await faceapi
            .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
            .withFaceLandmarks()
            .withAgeAndGender();

          if (canvas) {
            const ctx = canvas.getContext('2d');
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            
            const resizedDetections = faceapi.resizeResults(detections, {
              width: video.videoWidth,
              height: video.videoHeight
            });
            
            faceapi.draw.drawDetections(canvas, resizedDetections);
            faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
          }

          if (detections && detections.length > 0 && analyzing && !hasTrackedRef.current) {
            const detection = detections[0];
            const { age, gender, genderProbability } = detection;

            if (genderProbability > 0.7) {
              // ‚≠ê ‡∏ï‡∏±‡πâ‡∏á flag ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ã‡πâ‡∏≥
              hasTrackedRef.current = true;
              setAnalyzing(false);
              
              const profileData = {
                gender,
                age: Math.round(age),
                confidence: genderProbability
              };

              console.log('üë§ Face detected:', profileData);

              // ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å session ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
              try {
                await trackUserSession({
                  gender: gender,
                  age: Math.round(age)
                });

                console.log('‚úÖ Face data saved to Analytics');
              } catch (error) {
                console.error('‚ùå Error saving face data:', error);
              }

              setTimeout(() => {
                onAnalysisComplete(profileData);
              }, 1000);
            }
          }
        } catch (err) {
          console.error('Detection error:', err);
        }
      }
    };

    const interval = setInterval(detectFaces, 500);

    analysisTimeoutRef.current = setTimeout(() => {
      if (analyzing) {
        onClose();
      }
    }, 10000);

    return () => {
      clearInterval(interval);
      if (analysisTimeoutRef.current) {
        clearTimeout(analysisTimeoutRef.current);
      }
    };
  }, [modelsLoaded, isLoading, analyzing, onAnalysisComplete, onClose]);

  const getText = (key) => {
    const texts = {
      analyzing: { 
        th: '‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤...', 
        en: 'Analyzing face...', 
        zh: 'Ê≠£Âú®ÂàÜÊûêÈù¢ÈÉ®...', 
        ko: 'ÏñºÍµ¥ Î∂ÑÏÑù Ï§ë...' 
      },
      lookAtCamera: { 
        th: '‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏°‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏Å‡∏•‡πâ‡∏≠‡∏á', 
        en: 'Please look at camera', 
        zh: 'ËØ∑ÁúãÊëÑÂÉèÂ§¥', 
        ko: 'Ïπ¥Î©îÎùºÎ•º Î¥êÏ£ºÏÑ∏Ïöî' 
      }
    };
    return texts[key][language] || texts[key].th;
  };

  return (
    <div className="fixed inset-0 bg-black/95 z-50 flex items-center justify-center p-4">
      <button
        onClick={onClose}
        className="absolute top-8 right-8 text-white hover:text-red-500 transition z-10"
      >
        <X size={48} />
      </button>

      <div className="text-center">
        <div className="relative inline-block mb-8">
          {isLoading && (
            <div className="absolute inset-0 flex items-center justify-center bg-gray-900/50 rounded-3xl z-10">
              <Camera className="w-16 h-16 text-white animate-pulse" />
            </div>
          )}
          
          <div className="relative rounded-3xl overflow-hidden shadow-2xl border-4 border-white/20">
            <video
              ref={videoRef}
              autoPlay
              muted
              playsInline
              className="w-full max-w-4xl h-auto object-cover"
              style={{ maxHeight: '70vh' }}
            />
            <canvas
              ref={canvasRef}
              className="absolute top-0 left-0 w-full h-full"
            />
          </div>
        </div>

        <div className="text-white space-y-4">
          <h2 className="text-4xl font-bold">{getText('analyzing')}</h2>
          <p className="text-2xl text-white/80">{getText('lookAtCamera')}</p>
          <div className="flex justify-center gap-2 mt-6">
            <div className="w-4 h-4 bg-blue-500 rounded-full animate-bounce"></div>
            <div className="w-4 h-4 bg-blue-500 rounded-full animate-bounce" style={{ animationDelay: '0.1s' }}></div>
            <div className="w-4 h-4 bg-blue-500 rounded-full animate-bounce" style={{ animationDelay: '0.2s' }}></div>
          </div>
        </div>
      </div>
    </div>
  );
};

export default FaceAnalyzerPopup;